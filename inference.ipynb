{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler,UNet2DConditionModel,AutoencoderKL\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m7mde\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize CLIP tokenizer and text encoder\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "\n",
    "def get_text_embeddings(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=77).to(device)\n",
    "\n",
    "    # Pass through the text encoder\n",
    "    with torch.no_grad():\n",
    "        outputs = text_encoder(**inputs)\n",
    "\n",
    "    # Extract the last hidden state\n",
    "    last_hidden_state = outputs[0]\n",
    "\n",
    "    return last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "\n",
    "# Load a pretrained VAE\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(device)\n",
    "vae.eval()  # Set the VAE in evaluation mode\n",
    "\n",
    "def get_image_latent_dist(image):\n",
    "    with torch.no_grad():\n",
    "        latent_dist = vae.encode(image).latent_dist.sample()\n",
    "    return latent_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m7mde\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "model = UNet2DConditionModel(\n",
    "    sample_size=32,\n",
    "    in_channels=4,\n",
    "    out_channels=4,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(64, 128, 256),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "\n",
    "    ),\n",
    "\n",
    "    up_block_types=(\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    "    cross_attention_dim=768\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('unet_new.pth', weights_only=True))\n",
    "model.eval()\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_image_generation(model, vae, text, num_inference_steps, num_samples=1, guidance_scale=10):\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "    noise_scheduler.set_timesteps(num_inference_steps=num_inference_steps)\n",
    "    model.eval()\n",
    "    images_list = []\n",
    "    with torch.no_grad():\n",
    "        # Get text embeddings (conditional)\n",
    "        text_embeddings = get_text_embeddings(text)\n",
    "        \n",
    "        # Create unconditional embeddings (empty/neutral prompt)\n",
    "        unconditional_text_embeddings = torch.zeros_like(text_embeddings).to(device)\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            noise = torch.randn(1, 4, 32, 32).to(device)\n",
    "\n",
    "            for t in tqdm(noise_scheduler.timesteps, position=0, leave=True):\n",
    "                # Conditional prediction (with text)\n",
    "                noise_pred_cond = model(noise, t, text_embeddings).sample\n",
    "                \n",
    "                # Unconditional prediction (no text)\n",
    "                noise_pred_uncond = model(noise, t, unconditional_text_embeddings).sample\n",
    "                \n",
    "                # Classifier-Free Guidance: Combine both predictions\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "                \n",
    "                # Perform one step in the noise scheduler\n",
    "                noise = noise_scheduler.step(noise_pred, t, noise).prev_sample\n",
    "\n",
    "            # Decode the noise to generate images\n",
    "            noise = noise / 0.18215  # Undo the latent scaling\n",
    "            images = vae.decode(noise).sample\n",
    "            # images = vae(images).sample\n",
    "            \n",
    "            # Convert the image to numpy format for visualization\n",
    "            img = images.squeeze().cpu().permute(1, 2, 0).numpy()\n",
<<<<<<< HEAD
    "            img = (img + 1) / 2  # Normalize to [0, 1]\n",
    "            images_list.append((img * 255).astype('uint8'))  # Scale to [0, 255] for visualization\n",
    "            \n",
    "    return cv2.resize(images_list[0],(320,320))\n"
=======
    "            img = (img + 1) / 2 \n",
    "            images_list.append((img*255).astype('uint8'))\n",
    "    return images_list[0]"
>>>>>>> 87536c0a80461b2d81cce7ae3dbc18217695b15b
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.01it/s]\n"
     ]
    }
   ],
   "source": [
    "img = sample_image_generation(model,vae,\"this teen girl\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "IMPORTANT: You are using gradio version 3.50.0, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:02<00:00,  4.24it/s]\n",
      "100%|██████████| 11/11 [00:01<00:00,  5.55it/s]\n",
      "100%|██████████| 25/25 [00:04<00:00,  5.92it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  6.30it/s]\n",
      "100%|██████████| 40/40 [00:07<00:00,  5.47it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  6.28it/s]\n",
      "100%|██████████| 11/11 [00:01<00:00,  6.36it/s]\n",
      "100%|██████████| 11/11 [00:01<00:00,  6.14it/s]\n",
      "100%|██████████| 11/11 [00:01<00:00,  6.53it/s]\n",
      "100%|██████████| 14/14 [00:02<00:00,  5.82it/s]\n",
      "100%|██████████| 14/14 [00:02<00:00,  6.01it/s]\n",
      "100%|██████████| 14/14 [00:02<00:00,  6.19it/s]\n",
      "100%|██████████| 14/14 [00:02<00:00,  6.32it/s]\n",
      "100%|██████████| 14/14 [00:02<00:00,  6.01it/s]\n",
      "100%|██████████| 14/14 [00:02<00:00,  5.61it/s]\n",
      "100%|██████████| 14/14 [00:02<00:00,  5.47it/s]\n",
      "100%|██████████| 14/14 [00:02<00:00,  5.83it/s]\n",
      "100%|██████████| 14/14 [00:02<00:00,  5.96it/s]\n",
      "100%|██████████| 14/14 [00:02<00:00,  6.21it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  6.45it/s]\n",
      "100%|██████████| 25/25 [00:04<00:00,  6.21it/s]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Modify the generate function to accept both the prompt and number of steps\n",
    "def generate(prompt, num_inference_steps):\n",
    "    image = sample_image_generation(model, vae, prompt, num_inference_steps)  # Pass the prompt and number of steps\n",
    "    return image\n",
    "\n",
    "# Gradio Interface\n",
    "gr_interface = gr.Interface(\n",
    "    fn=generate,  # Function to generate images\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter your text prompt\"),  # Input prompt as free text\n",
    "        gr.Slider(label=\"Number of Inference Steps\", minimum=1, maximum=1000, step=1, value=11)  # Slider for steps\n",
    "    ],\n",
    "    outputs=gr.Image(label=\"Generated Image\"),  # Output the generated image\n",
    "    title=\"Text-to-Image Generator\",\n",
    "    description='''example usage: This female is blond Hair, Heavy Makeup, No Beard, Smiling, Young.'''\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "gr_interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
