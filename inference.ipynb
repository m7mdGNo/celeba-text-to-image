{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler,UNet2DConditionModel,AutoencoderKL\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize CLIP tokenizer and text encoder\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "\n",
    "def get_text_embeddings(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=77).to(device)\n",
    "\n",
    "    # Pass through the text encoder\n",
    "    with torch.no_grad():\n",
    "        outputs = text_encoder(**inputs)\n",
    "\n",
    "    # Extract the last hidden state\n",
    "    last_hidden_state = outputs[0]\n",
    "\n",
    "    return last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "\n",
    "# Load a pretrained VAE\n",
    "# vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device)\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(device)\n",
    "# vae.load_state_dict(torch.load('/kaggle/working/vae_step.pth', weights_only=True))\n",
    "vae.eval()  # Set the VAE in evaluation mode\n",
    "\n",
    "def get_image_latent_dist(image):\n",
    "    with torch.no_grad():\n",
    "        latent_dist = vae.encode(image).latent_dist.sample()\n",
    "    return latent_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DConditionModel(\n",
    "    sample_size=32,\n",
    "    in_channels=4,\n",
    "    out_channels=4,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(64, 128, 256),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "\n",
    "    ),\n",
    "\n",
    "    up_block_types=(\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    "    cross_attention_dim=768\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('unet_new.pth', weights_only=True))\n",
    "model.eval()\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_image_generation(model, vae, text, num_inference_steps, num_samples=1, guidance_scale=10):\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "    noise_scheduler.set_timesteps(num_inference_steps=num_inference_steps)\n",
    "    model.eval()\n",
    "    images_list = []\n",
    "    with torch.no_grad():\n",
    "        # Get text embeddings (conditional)\n",
    "        text_embeddings = get_text_embeddings(text)\n",
    "        \n",
    "        # Create unconditional embeddings (empty/neutral prompt)\n",
    "        unconditional_text_embeddings = torch.zeros_like(text_embeddings).to(device)\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            noise = torch.randn(1, 4, 32, 32).to(device)\n",
    "\n",
    "            for t in tqdm(noise_scheduler.timesteps, position=0, leave=True):\n",
    "                # Conditional prediction (with text)\n",
    "                noise_pred_cond = model(noise, t, text_embeddings).sample\n",
    "                \n",
    "                # Unconditional prediction (no text)\n",
    "                noise_pred_uncond = model(noise, t, unconditional_text_embeddings).sample\n",
    "                \n",
    "                # Classifier-Free Guidance: Combine both predictions\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "                \n",
    "                # Perform one step in the noise scheduler\n",
    "                noise = noise_scheduler.step(noise_pred, t, noise).prev_sample\n",
    "\n",
    "            # Decode the noise to generate images\n",
    "            noise = noise / 0.18215  # Undo the latent scaling\n",
    "            images = vae.decode(noise).sample\n",
    "            # images = vae(images).sample\n",
    "            \n",
    "            # Convert the image to numpy format for visualization\n",
    "            img = images.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "            img = (img + 1) / 2  # Normalize to [0, 1]\n",
    "            images_list.append((img * 255).astype('uint8'))  # Scale to [0, 255] for visualization\n",
    "            \n",
    "    return cv2.resize(images_list[0],(320,320))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Modify the generate function to accept both the prompt and number of steps\n",
    "def generate(prompt, num_inference_steps):\n",
    "    image = sample_image_generation(model, vae, prompt, num_inference_steps)  # Pass the prompt and number of steps\n",
    "    return image\n",
    "\n",
    "# Gradio Interface\n",
    "gr_interface = gr.Interface(\n",
    "    fn=generate,  # Function to generate images\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter your text prompt\"),  # Input prompt as free text\n",
    "        gr.Slider(label=\"Number of Inference Steps\", minimum=1, maximum=1000, step=1, value=11)  # Slider for steps\n",
    "    ],\n",
    "    outputs=gr.Image(label=\"Generated Image\"),  # Output the generated image\n",
    "    title=\"Text-to-Image Generator\",\n",
    "    description='''example usage: This female is blond Hair, Heavy Makeup, No Beard, Smiling, Young.'''\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "gr_interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
