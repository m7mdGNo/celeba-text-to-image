{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler,UNet2DConditionModel,AutoencoderKL\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m7mde\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize CLIP tokenizer and text encoder\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "\n",
    "def get_text_embeddings(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=77).to(device)\n",
    "\n",
    "    # Pass through the text encoder\n",
    "    with torch.no_grad():\n",
    "        outputs = text_encoder(**inputs)\n",
    "\n",
    "    # Extract the last hidden state\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "    return last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "\n",
    "# Load a pretrained VAE\n",
    "# vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device)\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(device)\n",
    "# vae.load_state_dict(torch.load('/kaggle/working/vae_step.pth', weights_only=True))\n",
    "vae.eval()  # Set the VAE in evaluation mode\n",
    "\n",
    "def get_image_latent_dist(image):\n",
    "    with torch.no_grad():\n",
    "        latent_dist = vae.encode(image).latent_dist.sample()\n",
    "    return latent_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m7mde\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "model = UNet2DConditionModel(\n",
    "    sample_size=32,\n",
    "    in_channels=4,\n",
    "    out_channels=4,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(64, 128, 256),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "\n",
    "    ),\n",
    "\n",
    "    up_block_types=(\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    "    cross_attention_dim=768\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('unet_new.pth', weights_only=True))\n",
    "model.eval()\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_image_generation(model, vae, text,num_inference_steps,num_samples=4):\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "    noise_scheduler.set_timesteps(num_inference_steps=num_inference_steps)\n",
    "    model.eval()\n",
    "    images_list = []\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = get_text_embeddings(text)\n",
    "        for i in range(num_samples):\n",
    "            noise = torch.randn(1, 4, 32, 32).to(device)\n",
    "\n",
    "            for t in tqdm(noise_scheduler.timesteps, position=0, leave=True):\n",
    "                noise_pred = model(noise, t, text_embeddings).sample\n",
    "                noise = noise_scheduler.step(noise_pred, t, noise).prev_sample\n",
    "\n",
    "            # Decode the noise to generate images\n",
    "            noise = noise / 0.18215  # Undo the latent scaling\n",
    "            images = vae.decode(noise).sample\n",
    "            \n",
    "            # images = vae(images).sample\n",
    "            img = images.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "            img = (img + 1) / 2 \n",
    "            images_list.append((img*255).astype('uint8'))\n",
    "    # compines images by nxm in one image\n",
    "    row1 = np.concatenate(images_list[:2],axis=0)\n",
    "    row2 = np.concatenate(images_list[2:],axis=0)\n",
    "    final_image = np.concatenate([row1,row2],axis=1)\n",
    "    return final_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.42it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.57it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.29it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.97it/s]\n"
     ]
    }
   ],
   "source": [
    "img = sample_image_generation(model,vae,\"girl\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "IMPORTANT: You are using gradio version 3.50.0, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 10.26it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 13.49it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 12.28it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 12.54it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 12.39it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 12.27it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 11.47it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 11.76it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 11.67it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 11.28it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 12.41it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 11.49it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 10.92it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 11.66it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 11.60it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 12.30it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 11.14it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 11.70it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 11.68it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 10.62it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 10.89it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 11.50it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 11.61it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 11.81it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 10.48it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 12.38it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 12.13it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 12.33it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 12.31it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 11.11it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 10.61it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 12.08it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12.56it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12.61it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12.99it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12.94it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.60it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.51it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12.23it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.03it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12.14it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12.17it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12.61it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12.65it/s]\n",
      "100%|██████████| 20/20 [00:01<00:00, 11.64it/s]\n",
      "100%|██████████| 20/20 [00:01<00:00, 11.32it/s]\n",
      "100%|██████████| 20/20 [00:01<00:00, 12.43it/s]\n",
      "100%|██████████| 20/20 [00:01<00:00, 12.29it/s]\n",
      "100%|██████████| 20/20 [00:01<00:00, 11.81it/s]\n",
      "100%|██████████| 20/20 [00:01<00:00, 11.73it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  9.96it/s]\n",
      "100%|██████████| 20/20 [00:01<00:00, 11.45it/s]\n",
      "100%|██████████| 20/20 [00:01<00:00, 11.49it/s]\n",
      "100%|██████████| 20/20 [00:01<00:00, 12.35it/s]\n",
      "100%|██████████| 20/20 [00:01<00:00, 12.76it/s]\n",
      "100%|██████████| 20/20 [00:01<00:00, 13.28it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 11.08it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 12.20it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 11.91it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 12.49it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 11.58it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 11.67it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.04it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.44it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 11.48it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 11.60it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 11.97it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.94it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 11.31it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.46it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.65it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.04it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 11.16it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.31it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 11.81it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.58it/s]\n",
      "100%|██████████| 23/23 [00:02<00:00, 10.98it/s]\n",
      "100%|██████████| 23/23 [00:02<00:00, 11.17it/s]\n",
      "100%|██████████| 23/23 [00:02<00:00, 10.95it/s]\n",
      "100%|██████████| 23/23 [00:02<00:00, 10.72it/s]\n",
      "100%|██████████| 23/23 [00:02<00:00, 10.68it/s]\n",
      "100%|██████████| 23/23 [00:02<00:00, 11.18it/s]\n",
      "100%|██████████| 23/23 [00:02<00:00, 11.16it/s]\n",
      "100%|██████████| 23/23 [00:01<00:00, 12.48it/s]\n",
      "100%|██████████| 23/23 [00:01<00:00, 11.82it/s]\n",
      "100%|██████████| 23/23 [00:01<00:00, 12.10it/s]\n",
      "100%|██████████| 23/23 [00:02<00:00, 11.41it/s]\n",
      "100%|██████████| 23/23 [00:01<00:00, 11.69it/s]\n",
      "100%|██████████| 23/23 [00:02<00:00,  9.41it/s]\n",
      "100%|██████████| 23/23 [00:02<00:00, 10.90it/s]\n",
      "100%|██████████| 23/23 [00:02<00:00,  9.60it/s]\n",
      "100%|██████████| 23/23 [00:01<00:00, 12.20it/s]\n",
      "100%|██████████| 23/23 [00:02<00:00, 10.65it/s]\n",
      "100%|██████████| 23/23 [00:02<00:00, 10.83it/s]\n",
      "100%|██████████| 23/23 [00:02<00:00, 10.20it/s]\n",
      "100%|██████████| 23/23 [00:01<00:00, 12.27it/s]\n",
      "100%|██████████| 18/18 [00:01<00:00,  9.50it/s]\n",
      "100%|██████████| 18/18 [00:01<00:00, 12.01it/s]\n",
      "100%|██████████| 18/18 [00:01<00:00, 12.25it/s]\n",
      "100%|██████████| 18/18 [00:01<00:00,  9.86it/s]\n",
      "100%|██████████| 18/18 [00:01<00:00, 10.31it/s]\n",
      "100%|██████████| 18/18 [00:01<00:00,  9.82it/s]\n",
      "100%|██████████| 18/18 [00:02<00:00,  7.89it/s]\n",
      "100%|██████████| 18/18 [00:01<00:00,  9.41it/s]\n",
      "100%|██████████| 30/30 [00:02<00:00, 10.35it/s]\n",
      "100%|██████████| 30/30 [00:03<00:00,  9.50it/s]\n",
      "100%|██████████| 30/30 [00:02<00:00, 11.88it/s]\n",
      "100%|██████████| 30/30 [00:02<00:00, 12.39it/s]\n",
      "100%|██████████| 40/40 [00:04<00:00,  9.21it/s]\n",
      "100%|██████████| 40/40 [00:03<00:00, 10.77it/s]\n",
      "100%|██████████| 40/40 [00:03<00:00, 10.82it/s]\n",
      "100%|██████████| 40/40 [00:04<00:00,  8.91it/s]\n",
      "100%|██████████| 70/70 [00:06<00:00, 10.73it/s]\n",
      "100%|██████████| 70/70 [00:06<00:00, 10.72it/s]\n",
      "100%|██████████| 70/70 [00:06<00:00, 10.70it/s]\n",
      "100%|██████████| 70/70 [00:06<00:00, 11.26it/s]\n",
      "100%|██████████| 70/70 [00:07<00:00,  9.95it/s]\n",
      "100%|██████████| 70/70 [00:06<00:00, 10.40it/s]\n",
      "100%|██████████| 70/70 [00:07<00:00,  9.58it/s]\n",
      "100%|██████████| 70/70 [00:06<00:00, 10.02it/s]\n",
      "100%|██████████| 21/21 [00:02<00:00, 10.15it/s]\n",
      "100%|██████████| 21/21 [00:02<00:00,  8.09it/s]\n",
      "100%|██████████| 21/21 [00:02<00:00,  9.68it/s]\n",
      "100%|██████████| 21/21 [00:01<00:00, 10.68it/s]\n",
      "100%|██████████| 22/22 [00:02<00:00,  9.84it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.61it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.23it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.19it/s]\n",
      "100%|██████████| 22/22 [00:02<00:00,  9.51it/s]\n",
      "100%|██████████| 22/22 [00:02<00:00,  9.20it/s]\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.43it/s]\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.49it/s]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Modify the generate function to accept both the prompt and number of steps\n",
    "def generate(prompt, num_inference_steps):\n",
    "    image = sample_image_generation(model, vae, prompt, num_inference_steps)  # Pass the prompt and number of steps\n",
    "    return image\n",
    "\n",
    "# Gradio Interface\n",
    "gr_interface = gr.Interface(\n",
    "    fn=generate,  # Function to generate images\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter your text prompt\"),  # Input prompt as free text\n",
    "        gr.Slider(label=\"Number of Inference Steps\", minimum=1, maximum=1000, step=1, value=50)  # Slider for steps\n",
    "    ],\n",
    "    outputs=gr.Image(label=\"Generated Image\"),  # Output the generated image\n",
    "    title=\"Text-to-Image Generator\",\n",
    "    description=\"Enter a text description, and adjust the number of inference steps for image generation.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "gr_interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
